{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mat-O-Lab/MSEO/blob/main/tools/csv_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T23aj7f1req6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Owlready2 in c:\\users\\richard\\anaconda3\\lib\\site-packages (0.34)\n"
     ]
    }
   ],
   "source": [
    "from numpy.core.numeric import NaN\n",
    "#-*- coding: UTF-8 -*-\n",
    "#@title Code { vertical-output: true, display-mode: \"form\" }\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import io\n",
    "import sys\n",
    "import ast, re\n",
    "import base64\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "from contextlib import redirect_stderr\n",
    "from csv import Sniffer\n",
    "import chardet \n",
    "%matplotlib notebook\n",
    "\n",
    "!pip install Owlready2\n",
    "from owlready2 import *\n",
    "\n",
    "#there is a bug in Owlready2 when having imports in turtle in a owl file\n",
    "# if the error is thrown, load again and it is fine\n",
    "try:\n",
    "  mseo=get_ontology(\"https://purl.matolab.org/mseo/mid\").load()\n",
    "except:\n",
    "  mseo=get_ontology(\"https://purl.matolab.org/mseo/mid\").load()\n",
    "  \n",
    "cco_mu=get_ontology(\"http://www.ontologyrepository.com/CommonCoreOntologies/Mid/UnitsOfMeasureOntology/\").load()\n",
    "qudt=get_ontology('http://www.qudt.org/qudt/owl/1.0.0/unit.owl').load()\n",
    "\n",
    "json_ld_context=[\"http://www.w3.org/ns/csvw\", {\n",
    "    \"cco\": \"http://www.ontologyrepository.com/CommonCoreOntologies/\",\n",
    "    \"mseo\": mseo.base_iri,\n",
    "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"}\n",
    "    ]\n",
    "\n",
    "def get_encoding(file_data):\n",
    "    \"\"\"\n",
    "\n",
    "    :param file_data:   content of the file we want to parse\n",
    "    :return:            encoding of the specified file content e.g. utf-8, ascii..\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    result = chardet.detect(file_data)\n",
    "    return result['encoding']\n",
    "\n",
    "def get_column_separator(file_data):\n",
    "  \"\"\"\n",
    "\n",
    "  :param file_data: data of the file we want to parse\n",
    "  :return:          the seperator of the specified data, e.g. \";\" or \",\"\n",
    "  \"\"\"\n",
    "  file_string = io.StringIO(file_data.decode(encoding.value)) \n",
    "  sniffer = Sniffer()\n",
    "  dialect = sniffer.sniff(file_string.read(512))\n",
    "  return dialect.delimiter\n",
    "\n",
    "def get_header_length(file_data, separator_string, encoding):\n",
    "\n",
    "  \"\"\" This method finds the beginning of a header line inside a csv file.\n",
    "        Some csv files begin with additional information before\n",
    "        displaying the actual data-table.\n",
    "\n",
    "        We want to solve this problem by finding the beginning of the header-line\n",
    "        (column-descriptors) and read the metainfo and data-table separately.\n",
    "\n",
    "  :param file_data: content of the file we want to parse\n",
    "  :param separator_string: csv-separator\n",
    "  :param encoding: text encoding\n",
    "  :return: a 2-tuple of (first_head_line, max_columns_additional_header)\n",
    "                where\n",
    "                    first_head_line : index of the header line in the csv file\n",
    "                    max_columns_additional_header : number of columns in the data-table\n",
    "  \"\"\"\n",
    "\n",
    "  # since pandas throws errormessages when encountering a parseerror (meaning when\n",
    "  # encountering a csv-file with changing column-count for example), we can\n",
    "  # redirect the error to file_string. Then, we can read and analyze the error-message.\n",
    "  # This is helpful since we can see in which line the parser expected n columns, but got m instead.\n",
    "  file_string = io.StringIO(file_data.decode(encoding))  \n",
    "  f = io.StringIO()\n",
    "  with redirect_stderr(f):\n",
    "      df = pd.read_csv(file_string,sep=separator.value,error_bad_lines=False,warn_bad_lines=True,header=None)\n",
    "  f.seek(0)\n",
    "  #without utf string code b' \n",
    "  warn_str=f.read()[2:-2]\n",
    "\n",
    "  # split the warnings up\n",
    "  warnlist=warn_str.split('\\\\n')[:-1]\n",
    "\n",
    "  # The warnings we care about are of form 'Skipping line x: expected n columns, got m'\n",
    "  # readout row index and column count in warnings\n",
    "  line_numbers=[int(re.search('Skipping line (.+?):', line).group(1)) for line in warnlist]\n",
    "\n",
    "  # get the found number of columns\n",
    "  column_numbers=[int(line[-1]) for line in warnlist]\n",
    "  column_numbersm1=column_numbers.copy()\n",
    "  if not column_numbersm1:\n",
    "    #no additional header\n",
    "    return 0,0\n",
    "\n",
    "  #pop last element, so column_numbers is always length +1\n",
    "  column_numbersm1.pop(-1)\n",
    "\n",
    "  #assumes that the file ends with a uniform table with constant column count\n",
    "  #determine changes in counted columns starting from the last line of file\n",
    "  changed_column_count_line=[ line_numbers[index+1] for index in reversed(range(len(column_numbersm1)))\n",
    "                              if column_numbersm1[index] != column_numbers[index+1]]\n",
    "\n",
    "  # if there are column count - changes, then the first head-line is the the index\n",
    "  # of the row of the last change of column count minus 1.\n",
    "  if changed_column_count_line:\n",
    "\n",
    "    # additional header has ends in line before the last change of column count\n",
    "    first_head_line=changed_column_count_line[0]-1\n",
    "  elif line_numbers:\n",
    "\n",
    "    # edgecase is that we only have one column-count change, in this case,\n",
    "    # changed_column_count_line is empty, thus, first_head_line is just the first change\n",
    "    first_head_line=line_numbers[0]-1\n",
    "  else:\n",
    "    first_head_line=0\n",
    "\n",
    "  # starting from first_head_line, max_columns_additional_header is the\n",
    "  # maximum number of columns\n",
    "  max_columns_additional_header = max(column_numbers[:line_numbers.index(first_head_line+1)-1])\n",
    "  return first_head_line, max_columns_additional_header\n",
    "\n",
    "\n",
    "def get_num_header_rows_and_dataframe(file_data, separator_string, header_length, encoding):\n",
    "  \"\"\"\n",
    "\n",
    "  :param file_data: content of the file we want to parse\n",
    "  :param separator_string: csv-delimiter\n",
    "  :param header_length: rows of the header\n",
    "  :param encoding: csv-encoding\n",
    "  :return: 2-tuple (num_header_rows, table_data)\n",
    "                where\n",
    "                    num_header_rows : number of header rows\n",
    "                    table_data : pandas DataFrame object containing the tabular information\n",
    "  \"\"\"\n",
    "  file_string = io.StringIO(file_data.decode(encoding))\n",
    "  num_header_rows=1\n",
    "\n",
    "  good_readout=False\n",
    "\n",
    "  # loop until we find a row which contains non-text values.\n",
    "  # we do this, because there may be multiple additional header lines below the original header line.\n",
    "  while not good_readout:\n",
    "    file_string.seek(0)\n",
    "\n",
    "    table_data = pd.read_csv(file_string,header=list(range(num_header_rows)),sep=separator_string,skiprows=header_length,encoding=encoding)\n",
    "    #test if all text values in first table row -> is a second header row\n",
    "    all_text=all([get_value_type(value)=='TEXT' for column,value in table_data.iloc[0].items()])\n",
    "    if all_text:\n",
    "      num_header_rows+=1\n",
    "      continue\n",
    "    else:\n",
    "      good_readout=True\n",
    "  return num_header_rows, table_data\n",
    "\n",
    "def get_unit(string):\n",
    "  found=list(cco_mu.search(alternative_label=string))\\\n",
    "          +list(cco_mu.search(SI_unit_symbol=string))\\\n",
    "          +list(mseo.search(alternative_label=string))\\\n",
    "          +list(mseo.search(SI_unit_symbol=string))\\\n",
    "          +list(qudt.search(symbol=string))\\\n",
    "          +list(qudt.search(abbreviation=string))\\\n",
    "          +list(qudt.search(ucumCode=string))\n",
    "  if found:\n",
    "    return {\"cco:uses_measurement_unit\": {\"@id\": str(found[0].iri), \"@type\": str(found[0].is_a)}}\n",
    "  else:\n",
    "    return {}\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def get_value_type(string):\n",
    "    string=str(string)\n",
    "    #remove spaces and replace , with . and\n",
    "    string=string.strip().replace(',','.')\n",
    "    if len(string) == 0: return 'BLANK'\n",
    "    try:\n",
    "        t=ast.literal_eval(string)\n",
    "    except ValueError:\n",
    "        return 'TEXT'\n",
    "    except SyntaxError:\n",
    "        if is_date(string):\n",
    "          return 'DATE'\n",
    "        else:\n",
    "          return 'TEXT'\n",
    "    else:\n",
    "        if type(t) in [int, float, bool]:\n",
    "          if type(t) is int:\n",
    "              return 'INT'\n",
    "          if t in set((True,False)):\n",
    "              return 'BOOL'\n",
    "          if type(t) is float:\n",
    "              return 'FLOAT'\n",
    "        else:\n",
    "            return 'TEXT' \n",
    "\n",
    "def describe_value(value_string):\n",
    "  if pd.isna(value_string):\n",
    "    return {}\n",
    "  elif get_value_type(value_string)=='INT':\n",
    "    return {'cco:has_integer_value': {'@value':value_string, '@type': 'xsd:integer'}}\n",
    "  elif get_value_type(value_string)=='BOOL':\n",
    "    return {'cco:has_bolean_value': {'@value':value_string, '@type': 'xsd:boolean'}}\n",
    "  elif get_value_type(value_string)=='FLOAT':\n",
    "    return {'cco:has_decimal_value': {'@value':value_string, '@type': 'xsd:decimal'}}\n",
    "  elif get_value_type(value_string)=='DATE':\n",
    "    return {'cco:has_datetime_value': {'@value':str(parse(value_string)), '@type': 'xsd:dateTime'}}\n",
    "  else:\n",
    "    # check if its a unit\n",
    "    unit_dict=get_unit(value_string)\n",
    "    if unit_dict:\n",
    "      return unit_dict\n",
    "    else:\n",
    "      return {'cco:has_text_value': {'@value':value_string, '@type': 'xsd:string'}}\n",
    "\n",
    "umlaute_dict = {\n",
    "    '\\u00e4': 'ae',  # U+00E4\t   \\xc3\\xa4\n",
    "    '\\u00f6': 'oe',  # U+00F6\t   \\xc3\\xb6\n",
    "    '\\u00fc': 'ue',  # U+00FC\t   \\xc3\\xbc\n",
    "    '\\u00c4': 'Ae',  # U+00C4\t   \\xc3\\x84\n",
    "    '\\u00d6': 'Oe',  # U+00D6\t   \\xc3\\x96\n",
    "    '\\u00dc': 'Ue',  # U+00DC\t   \\xc3\\x9c\n",
    "    '\\u00df': 'ss',  # U+00DF\t   \\xc3\\x9f\n",
    "}\n",
    "\n",
    "def make_id(string,namespace=None):\n",
    "  for k in umlaute_dict.keys():\n",
    "        string = string.replace(k, umlaute_dict[k])\n",
    "  if namespace:\n",
    "    return namespace+':'+re.sub('[^A-ZÜÖÄa-z0-9]+', '', string.title().replace(\" \", \"\"))\n",
    "  else:\n",
    "    return './'+re.sub('[^A-ZÜÖÄa-z0-9]+', '', string.title().replace(\" \", \"\"))\n",
    "\n",
    "def get_additional_header(file_data,separator,encoding):\n",
    "  \"\"\"\n",
    "\n",
    "  :param file_data: content of the file we want to parse\n",
    "  :param separator: csv-separator\n",
    "  :param encoding: text encoding\n",
    "  :return:\n",
    "  \"\"\"\n",
    "\n",
    "  # get length of additional header\n",
    "  header_length, max_columns_additional_header=get_header_length(file_data,separator,encoding)\n",
    "  if header_length:\n",
    "\n",
    "    file_string = io.StringIO(file_data.decode(encoding))\n",
    "    header_data = pd.read_csv(file_string,header=None,sep=separator,nrows=header_length,names=range(max_columns_additional_header),encoding=encoding,skip_blank_lines=False)\n",
    "    header_data['row']=header_data.index\n",
    "    header_data.rename(columns={0: 'param'}, inplace=True)\n",
    "    header_data.set_index('param',inplace=True)\n",
    "    header_data=header_data[~header_data.index.duplicated()]\n",
    "    header_data.dropna(thresh=2, inplace=True)\n",
    "    return header_data, header_length\n",
    "  else:\n",
    "    return None, 0\n",
    "\n",
    "\n",
    "def serialize_header(header_data,file_namespace=None):\n",
    "  params=list()\n",
    "  info_line_iri=\"cco:InformationLine\"\n",
    "  for parm_name, data in header_data.to_dict(orient='index').items():\n",
    "    #describe_value(data['value'])\n",
    "    para_dict={'@id': make_id(parm_name,file_namespace),'label':parm_name,'@type': info_line_iri}\n",
    "    for col_name, value in data.items():\n",
    "      #print(parm_name,col_name, value)\n",
    "      if col_name=='row':\n",
    "        para_dict['mseo:has_row_index']={\"@value\": data['row'],\"@type\": \"xsd:integer\"}\n",
    "      else:\n",
    "        para_dict={**para_dict,**describe_value(value)}\n",
    "    params.append(para_dict)\n",
    "  #print(params)\n",
    "  return params\n",
    "  \n",
    "\n",
    "def process_file(file_name,file_data,separator,encoding):\n",
    "  \"\"\"\n",
    "\n",
    "  :param file_name: name of the file we want to process\n",
    "  :param file_data: content of the file\n",
    "  :param separator: csv-seperator /delimiter\n",
    "  :param encoding:  text-encoding (e.g. utf-8..)\n",
    "  :return: a 2-tuple (meta_filename,result)\n",
    "                where\n",
    "                    result :    the resulting metadata on how to\n",
    "                                read the file (skiprows, colnames ..)\n",
    "                                as a json dump\n",
    "                    meta_filename :  the name of the metafile we want to write\n",
    "  \"\"\"\n",
    "\n",
    "  #init results dict\n",
    "  data_root_url=\"https://github.com/Mat-O-Lab/resources/\"\n",
    "\n",
    "  file_namespace=None\n",
    "  metadata_csvw = dict()\n",
    "  metadata_csvw[\"@context\"]=json_ld_context\n",
    "\n",
    "  metadata_csvw[\"url\"]=file_name\n",
    "\n",
    "  # read additional header lines and provide as meta in results dict\n",
    "  header_data, header_length=get_additional_header(file_data,separator,encoding)\n",
    "\n",
    "  if header_length:\n",
    "    #print(\"serialze additinal header\")\n",
    "    metadata_csvw[\"notes\"]=serialize_header(header_data,file_namespace)\n",
    "\n",
    "  # read tabular data structure, and determine number of header lines for column description used\n",
    "  header_lines, table_data=get_num_header_rows_and_dataframe(file_data,separator,header_length,encoding)\n",
    "\n",
    "  # describe dialect\n",
    "  metadata_csvw[\"dialect\"]={\"delimiter\": separator,\n",
    "  \"skipRows\": header_length, \"headerRowCount\": header_lines, \"encoding\": encoding}\n",
    "\n",
    "  # describe columns\n",
    "  if header_lines==1:\n",
    "    # see if there might be a unit string at the end of each title\n",
    "    # e.g. \"E_y (MPa)\"\n",
    "    column_json=list()\n",
    "    for index, title in enumerate(table_data.columns):\n",
    "      if len(title.split(' '))>1:\n",
    "        unit_json=get_unit(title.split(' ')[-1])  \n",
    "      else:\n",
    "        unit_json={}\n",
    "      json_str={**{'titles': title,'@id': make_id(title), \"@type\": \"Column\"},**unit_json}\n",
    "      column_json.append(json_str)\n",
    "    metadata_csvw[\"tableSchema\"]={\"columns\":column_json}\n",
    "\n",
    "  else:\n",
    "    column_json=list()\n",
    "\n",
    "    for index, (title,unit_str) in enumerate(table_data.columns):\n",
    "      json_str={**{'titles': title,'@id': make_id(title), \"@type\": \"Column\"},**get_unit(unit_str)}\n",
    "      column_json.append(json_str)\n",
    "\n",
    "    metadata_csvw[\"tableSchema\"]={\"columns\":column_json}\n",
    "  result=json.dumps(metadata_csvw, indent = 4)\n",
    "  meta_file_name = file_name.split(sep='.')[0] + '-metadata.json'\n",
    "  return meta_file_name, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "H2XGNfLRQ6Ai",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#@title Dialog { vertical-output: true }\n",
    "# dialog\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False,  # True to accept multiple files upload else False\n",
    "    description='Upload'\n",
    "\n",
    ")\n",
    "clear = widgets.Button(description='Clear!', layout=widgets.Layout(width='100px')); \n",
    "def on_clear(_):\n",
    "  uploader.value.clear()\n",
    "  uploader._counter = 0\n",
    "clear.on_click(on_clear)\n",
    "\n",
    "file= widgets.HBox([widgets.Label(value=\"File:\"), uploader,clear])\n",
    "encoding = widgets.Dropdown(\n",
    "    options=['auto', 'ISO-8859-1', 'UTF-8', 'ascii', 'latin-1','cp273'],\n",
    "    value='auto',\n",
    "    description='Encoding:',\n",
    "    disabled=False,\n",
    ")\n",
    "separator = widgets.Dropdown(\n",
    "    options=['auto', ',',';', '\\t', '|', \"\\s+\",\"\\s+|\\t+|\\s+\\t+|\\t+\\s+\"],\n",
    "    value='auto',\n",
    "    description='separator:',\n",
    "    disabled=False,\n",
    ")\n",
    "settings= widgets.HBox([encoding, separator])\n",
    "button = widgets.Button(description='Process!', layout=widgets.Layout(width='200px')); \n",
    "out = widgets.Output()\n",
    "\n",
    "def on_button_clicked(_):\n",
    "  # \"linking function with output\"\n",
    "  with out:\n",
    "    # what happens when we press the button\n",
    "    clear_output()\n",
    "    if not uploader.value.keys():\n",
    "      print('pls upload a file first')\n",
    "      return\n",
    "\n",
    "    # always grab the first file uploaded\n",
    "    input_file=uploader.value[list(uploader.value.keys())[0]]\n",
    "\n",
    "\n",
    "    # get the files name and content\n",
    "    file_name = input_file['metadata']['name']\n",
    "    file_data = input_file['content']\n",
    "\n",
    "    # if user chose auto-encoding, try to find out the encoding\n",
    "    if encoding.value=='auto':\n",
    "      encoding.value=get_encoding(file_data)\n",
    "\n",
    "    # if user chose auto-seperator, try to find the fitting seperator\n",
    "    if separator.value=='auto':\n",
    "      try:\n",
    "        separator.value=get_column_separator(file_data)\n",
    "      except:\n",
    "        print('cant find separator, pls manualy select')\n",
    "\n",
    "    # process the file\n",
    "    metafile_name, result =process_file(file_name,file_data,separator.value,encoding.value)\n",
    "    print(result)\n",
    "    res = result\n",
    "    b64 = base64.b64encode(res.encode())\n",
    "    payload = b64.decode()\n",
    "    html_buttons = '''<html>\n",
    "    <head>\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "    </head>\n",
    "    <body>\n",
    "    <a download=\"{filename}\" href=\"data:text/json;base64,{payload}\" download>\n",
    "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
    "    </a>\n",
    "    </body>\n",
    "    </html>\n",
    "    '''\n",
    "    html_button = html_buttons.format(payload=payload,filename=metafile_name)\n",
    "    display(widgets.HTML(html_button))\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "process = widgets.VBox([button,out])\n",
    "display(file,settings,process)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "csv_parser.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-9f97e93f",
   "language": "python",
   "display_name": "PyCharm (MSEO)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}